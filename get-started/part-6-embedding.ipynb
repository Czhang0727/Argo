{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This note will focus on embedding\n",
    "\n",
    "In transformer, the final embedding will be token-embedding and position-embedding.\n",
    "\n",
    "For transformer, the encoding use specific process like:\n",
    "\n",
    "<img src=\"./assert/position-embedding.png\" width=\"50%\" alt=\"Position Embedding\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = torch.nn.Embedding(max_len, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        position = torch.arange(seq_len).expand(x.shape[0], seq_len).to(x.device)\n",
    "        return self.token_embedding(x) + self.position_embedding(position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we improve embedding\n",
    "\n",
    "Depends on the task of embedding, we can set up different goal for embeeding.\n",
    "\n",
    "A common understanding is, since embedding is from \"token\" -> vector, we can use a hashmap save all results, so we could save bunch of time.\n",
    "\n",
    "This solution certainly contains some down effect, given text (which is hard to decide the splition). But the result should remains static.\n",
    "\n",
    "We could also compress the model size by group similar meaning tokens into same vector, for example, \"sun\" and \"solar\" and \"æ—¥\", could be same meaning, it will be easier for nesting.\n",
    "\n",
    "## Embedding out of language optimization\n",
    "\n",
    "Embedding also widely used recommandation as well, which represent an \"entity\" like user or item.\n",
    "\n",
    "In this case, most time we can perform a embeddingBag process. This is helpful to embedding for \"similar items\" or \"similar users\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
