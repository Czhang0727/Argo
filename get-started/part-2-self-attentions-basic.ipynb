{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is attention?\n",
    "\n",
    "Similar to we human-beings, attention is \"How much we should focus on an element\".\n",
    "\n",
    "Consider we have an array [1, 2, 3, 4, 5], and we have 100% focus on this array. When we dig into each element, we could split our focus like:\n",
    "\n",
    "[0.1, 0.2, 0.5, 0.1, 0.1]\n",
    "\n",
    "In this example, value 3 has most attention.\n",
    "\n",
    "## How we represent the attention\n",
    "\n",
    "attention chould be calculated from an input -> target processed with a weight. Take an example, if we concat user embedding and an item embedding, and map to how an user could related to an item. The output we are looking for will be the attention for user -> item\n",
    "\n",
    "$$\n",
    "attention = (input -> target) \\cdot weight\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's talk about self-attention\n",
    "\n",
    "Self attention is a unique condition, where the input and target is same object.\n",
    "\n",
    "Take a look at following fomular:\n",
    "\n",
    "$$\n",
    "Softmax(X \\cdot X.T) X\n",
    "$$\n",
    "\n",
    "Let's understand this step by step:\n",
    "\n",
    "1. $X \\cdot X.T$ will calculate the heapmap of how x focus on itself\n",
    "\n",
    "| X | a | b | c |\n",
    "|---|---|---|---|\n",
    "| a | 2 | 1 | 4 |\n",
    "| b | 3 | 3 | 1 |\n",
    "| c | 8 | 1 | 2 |\n",
    "\n",
    "2. Softmax will apply a normalization for this table, and make sum of all elments into 1\n",
    "3. for each attention after softmax, we process another dot process, as we discussed above, dot product can represent internal relationship between two things.\n",
    "4. After all these process, we are going to have a attention tensor, the higher score we have, it means the higher relation they have\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention in NN in action\n",
    "\n",
    "![self-attention](./assert/self-attentions.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5887, 2.5548, 1.3881],\n",
      "        [1.6586, 2.6938, 1.5145],\n",
      "        [1.6646, 2.6732, 1.4455]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/0_1k_yrs6k54bd9t5328d9n40000gn/T/ipykernel_44558/4037198026.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention = softmax(query @ key.T / math.sqrt(key.shape[1]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import math\n",
    "\n",
    "input = torch.tensor([\n",
    "    [1, 0, 1, 0],  # input 1\n",
    "    [0, 2, 0, 2],  # input 2\n",
    "    [1, 1, 1, 1]   # input 3\n",
    "], dtype=torch.float32)\n",
    "\n",
    "w_query = torch.rand(4, 3, dtype=torch.float32)\n",
    "w_key = torch.rand(4, 3, dtype=torch.float32)\n",
    "w_value = torch.rand(4, 3, dtype=torch.float32)\n",
    "\n",
    "query = input @ w_query\n",
    "key = input @ w_key\n",
    "value = input @ w_value\n",
    "\n",
    "attention = softmax(query @ key.T / math.sqrt(key.shape[1]))\n",
    "\n",
    "output = attention @ value\n",
    "\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
