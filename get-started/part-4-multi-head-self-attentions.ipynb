{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is multi-head attention\n",
    "\n",
    "Multi-head attention showed as below image\n",
    "\n",
    "<img src=\"./assert/multi-head-attentions.png\" width=\"400\" height=\"400\" alt=\"MHA\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain\n",
    "\n",
    "Multi-head attention is pretty similar to self-attention.\n",
    "\n",
    "On top of 1D query, key, value, now we have a n-d query, key, value.\n",
    "After attention process, we simply concat them and flat into a single array for output.\n",
    "\n",
    "## Why?\n",
    "\n",
    "Single-head attention only apply for \"one kind of understanding\", for example, single head will understand \"1 + 1\" as **plain text**.\n",
    "\n",
    "While multi-head attention could read \"1 + 1\" as both **plain text** and **formular**.\n",
    "\n",
    "The more head we have, the more rich context we could extract from the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.w_query = nn.Linear(d_model, d_model)\n",
    "        self.w_key = nn.Linear(d_model, d_model)\n",
    "        self.w_value = nn.Linear(d_model, d_model)\n",
    "        self.attention_scores = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       seq_len = x.shape[0]\n",
    "       query = self.w_query(x)\n",
    "       key = self.w_key(x)\n",
    "       value = self.w_value(x)\n",
    "       \n",
    "       # Reshape to (seq_len, n_heads, head_dim), then cast to (n_heads, seq_len, head_dim)\n",
    "       query = query.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "       key = key.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "       value = value.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "       \n",
    "       # score: (n_heads, seq_len, seq_len)\n",
    "       score = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(self.head_dim)\n",
    "       \n",
    "       # attention: (n_heads, seq_len, head_dim)\n",
    "       attention = torch.bmm(torch.softmax(score, dim=-1), value)\n",
    "       \n",
    "       attention =attention.transpose(0, 1).contiguous().view(seq_len, self.d_model)\n",
    "       return self.attention_scores(attention)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9528\n",
      "Epoch 10, Loss: 0.8633\n",
      "Epoch 20, Loss: 0.7874\n",
      "Epoch 30, Loss: 0.6941\n",
      "Epoch 40, Loss: 0.5665\n",
      "Epoch 50, Loss: 0.4330\n",
      "Epoch 60, Loss: 0.3291\n",
      "Epoch 70, Loss: 0.2463\n",
      "Epoch 80, Loss: 0.1821\n",
      "Epoch 90, Loss: 0.1270\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "embed_dim = 32\n",
    "n_heads = 4\n",
    "seq_len = 8\n",
    "\n",
    "x = torch.randn(seq_len, embed_dim)\n",
    "target = torch.randn(seq_len, embed_dim)\n",
    "\n",
    "model = MultiHeadAttention(embed_dim, n_heads)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add musk function to multi-head attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.w_query = nn.Linear(d_model, d_model)\n",
    "        self.w_key = nn.Linear(d_model, d_model)\n",
    "        self.w_value = nn.Linear(d_model, d_model)\n",
    "        self.attention_scores = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        seq_len = query.shape[0]\n",
    "        # from input, the query, key, value will be simply input matrix, input, input, input.\n",
    "        query = self.w_query(query)\n",
    "        key = self.w_key(key)\n",
    "        value = self.w_value(value)\n",
    "\n",
    "        query = query.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "        key = key.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "        value = value.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        attention_scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # The computation is pretty similar to the previous one:\n",
    "            # we have seq_len of query ( sub matrix ), after processing query * key.T, we get\n",
    "            # seq_len, (n_heads, n_heads), where n_head is the size of original attention matrix\n",
    "            \n",
    "            # Note:\n",
    "            # Post mask, we don't want to have 0 as masked value,\n",
    "            # because, softmax(0) = 1, which will make the attention score too high.\n",
    "            # So, we use -inf to mask the value. The normalization will still remains.\n",
    "            attention_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attention = torch.matmul(torch.softmax(attention_scores, dim=-1), value)\n",
    "\n",
    "        attention = attention.transpose(0, 1).contiguous().view(seq_len, self.d_model)\n",
    "        return self.attention_scores(attention)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9432\n",
      "Epoch 10, Loss: 0.8329\n",
      "Epoch 20, Loss: 0.7072\n",
      "Epoch 30, Loss: 0.5641\n",
      "Epoch 40, Loss: 0.4373\n",
      "Epoch 50, Loss: 0.3366\n",
      "Epoch 60, Loss: 0.2433\n",
      "Epoch 70, Loss: 0.1577\n",
      "Epoch 80, Loss: 0.0930\n",
      "Epoch 90, Loss: 0.0512\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "embed_dim = 32\n",
    "n_heads = 4\n",
    "seq_len = 8\n",
    "\n",
    "x = torch.randn(seq_len, embed_dim)\n",
    "target = torch.randn(seq_len, embed_dim)\n",
    "\n",
    "# mask: (seq_len, seq_len)\n",
    "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "\n",
    "# model: (seq_len, embed_dim) -> (seq_len, embed_dim)\n",
    "model = MaskedMultiHeadAttention(embed_dim, n_heads)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x, x, x, mask)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# In this example, we want to support batch processing.\n",
    "# Means, the input will be now (batch_size, seq_size, d_model)\n",
    "\n",
    "# Note:\n",
    "# Batch will be as same as original output.\n",
    "# The reason we need batch is because we want to support parallel processing.\n",
    "# Consider GPU can handle larger memory, we can process more data in parallel.\n",
    "\n",
    "class BatchedMaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.w_query = nn.Linear(d_model, d_model)\n",
    "        self.w_key = nn.Linear(d_model, d_model)\n",
    "        self.w_value = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attention_scores = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "        seq_size = query.shape[1]\n",
    "        \n",
    "        # size for query, key, value: (batch_size, seq_size, d_model)\n",
    "        query = self.w_query(query)\n",
    "        key = self.w_key(key)\n",
    "        value = self.w_value(value)\n",
    "       \n",
    "        # cast size to (batch_size, n_heads, seq_size, head_dim)\n",
    "        query = query.view(batch_size, seq_size, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size, seq_size, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, seq_size, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # the attention score will be computed from (seq_size, head_dim) * (seq_size, head_dim).T\n",
    "        # attention_scores: (batch_size, n_heads, seq_size, seq_size)\n",
    "        attention_scores = torch.matmul(query, key.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            # following previous steps to calcuation masked value.\n",
    "            attention_scores.masked_fill(mask== 0, float(\"-inf\"))\n",
    "\n",
    "        # matmul with value for final score.\n",
    "        # attention_scores: (batch_size, n_heads, seq_size, seq_size) * (batch_size, n_heads, seq_size, head_dim) -> (batch_size, n_heads, seq_size, head_dim)\n",
    "        attention_scores = torch.matmul(torch.softmax(attention_scores, dim=-1), value)\n",
    "    \n",
    "        # cast size to original format: (batch_size, n_heads, seq_size, head_dim) -> (batch_size, seq_size, n_heads, head_dim) -> (batch_size, seq_size, d_model)\n",
    "        # Note:\n",
    "        # 1. transpose(1, 2) is to move the n_heads dimension to the last dimension.\n",
    "        # 2. contiguous() is to make the tensor contiguous in memory.\n",
    "        # 3. view(batch_size, seq_size, self.d_model) is to reshape the tensor to the original size.\n",
    "        attention_output = attention_scores.transpose(1, 2).contiguous().view(batch_size, seq_size, self.d_model)\n",
    "\n",
    "        return self.attention_scores(attention_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9783\n",
      "Epoch 10, Loss: 0.9198\n",
      "Epoch 20, Loss: 0.8733\n",
      "Epoch 30, Loss: 0.8275\n",
      "Epoch 40, Loss: 0.7754\n",
      "Epoch 50, Loss: 0.7139\n",
      "Epoch 60, Loss: 0.6447\n",
      "Epoch 70, Loss: 0.5727\n",
      "Epoch 80, Loss: 0.5028\n",
      "Epoch 90, Loss: 0.4369\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 4\n",
    "embed_dim = 32\n",
    "n_heads = 4\n",
    "seq_len = 8\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "target = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "# mask: (batch_size, seq_len, seq_len)\n",
    "mask = torch.triu(torch.ones(batch_size, seq_len, seq_len), diagonal=1)\n",
    "\n",
    "# model: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
    "model = BatchedMaskedMultiHeadAttention(embed_dim, n_heads)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x, x, x, mask)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking\n",
    "\n",
    "1. Since we are slicing by **head_num**, why not just random pick like dropout?\n",
    "\n",
    "Well, actually in middle of MHA, order contains meaning, it called \"position_embedding\", without adding more training cycles to trace the order, we pretty much randomize the parameters again.\n",
    "\n",
    "2. Also, check size carefully, since the computation of matrix will always apply on (A, B) @ (B, A), we need mention in matrix computation process, which two dimention we will need to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
