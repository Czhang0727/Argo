{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is multi-head attention\n",
    "\n",
    "Multi-head attention showed as below image\n",
    "\n",
    "<img src=\"./assert/multi-head-attentions.png\" width=\"400\" height=\"400\" alt=\"MHA\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain\n",
    "\n",
    "Multi-head attention is pretty similar to self-attention.\n",
    "\n",
    "On top of 1D query, key, value, now we have a n-d query, key, value.\n",
    "After attention process, we simply concat them and flat into a single array for output.\n",
    "\n",
    "## Why?\n",
    "\n",
    "Single-head attention only apply for \"one kind of understanding\", for example, single head will understand \"1 + 1\" as **plain text**.\n",
    "\n",
    "While multi-head attention could read \"1 + 1\" as both **plain text** and **formular**.\n",
    "\n",
    "The more head we have, the more rich context we could extract from the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.w_query = nn.Linear(d_model, d_model)\n",
    "        self.w_key = nn.Linear(d_model, d_model)\n",
    "        self.w_value = nn.Linear(d_model, d_model)\n",
    "        self.attention_scores = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       seq_len = x.shape[0]\n",
    "       query = self.w_query(x)\n",
    "       key = self.w_key(x)\n",
    "       value = self.w_value(x)\n",
    "       \n",
    "       # Reshape to (seq_len, n_heads, head_dim), then cast to (n_heads, seq_len, head_dim)\n",
    "       query = query.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "       key = key.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "       value = value.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "       \n",
    "       # score: (n_heads, seq_len, seq_len)\n",
    "       score = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(self.head_dim)\n",
    "       \n",
    "       # attention: (n_heads, seq_len, head_dim)\n",
    "       attention = torch.bmm(torch.softmax(score, dim=-1), value)\n",
    "       \n",
    "       attention =attention.transpose(0, 1).contiguous().view(seq_len, self.d_model)\n",
    "       return self.attention_scores(attention)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9528\n",
      "Epoch 10, Loss: 0.8633\n",
      "Epoch 20, Loss: 0.7874\n",
      "Epoch 30, Loss: 0.6941\n",
      "Epoch 40, Loss: 0.5665\n",
      "Epoch 50, Loss: 0.4330\n",
      "Epoch 60, Loss: 0.3291\n",
      "Epoch 70, Loss: 0.2463\n",
      "Epoch 80, Loss: 0.1821\n",
      "Epoch 90, Loss: 0.1270\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "embed_dim = 32\n",
    "n_heads = 4\n",
    "seq_len = 8\n",
    "\n",
    "# Fix: Correct input shape (seq_len, d_model)\n",
    "x = torch.randn(seq_len, embed_dim)\n",
    "target = torch.randn(seq_len, embed_dim)\n",
    "\n",
    "model = MultiHeadAttention(embed_dim, n_heads)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
