{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer in image\n",
    "\n",
    "<img src=\"./assert/transformer.png\" width=\"50%\" height=\"50%\" alt=\"transformer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add musk function to multi-head attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.w_query = nn.Linear(d_model, d_model)\n",
    "        self.w_key = nn.Linear(d_model, d_model)\n",
    "        self.w_value = nn.Linear(d_model, d_model)\n",
    "        self.attention_scores = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        seq_len = query.shape[0]\n",
    "        # from input, the query, key, value will be simply input matrix, input, input, input.\n",
    "        query = self.w_query(query)\n",
    "        key = self.w_key(key)\n",
    "        value = self.w_value(value)\n",
    "\n",
    "        query = query.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "        key = key.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "        value = value.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        attention_scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # The computation is pretty similar to the previous one:\n",
    "            # we have seq_len of query ( sub matrix ), after processing query * key.T, we get\n",
    "            # seq_len, (n_heads, n_heads), where n_head is the size of original attention matrix\n",
    "            \n",
    "            # Note:\n",
    "            # Post mask, we don't want to have 0 as masked value,\n",
    "            # because, softmax(0) = 1, which will make the attention score too high.\n",
    "            # So, we use -inf to mask the value. The normalization will still remains.\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attention = torch.matmul(torch.softmax(attention_scores, dim=-1), value)\n",
    "\n",
    "        attention = attention.transpose(0, 1).contiguous().view(seq_len, self.d_model)\n",
    "        return self.attention_scores(attention)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a component for add & Norm, it pretty much means x + layer_norm(x)\n",
    "\n",
    "class TransformerAddNorm(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    # Note the sublayer here is MultiHeadAttention\n",
    "    def forward(self, x, sublayer):\n",
    "        return self.layer_norm(x + sublayer(x))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.feed_forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # initialize position encoding, shape: (max_len, d_model), all zeros\n",
    "        self.position_encoding = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # position: (max_len, 1), this will be position for each token\n",
    "        self.position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # i will be step_count we used to compute div_term\n",
    "        i = torch.arange(0, d_model, step=2, dtype=torch.float)\n",
    "        \n",
    "        # the calculation of div_term is based on the formula in the paper, check image above\n",
    "        # we apply a boardcast to the position matrix to get a matrix with shape (max_len, d_model / 2)\n",
    "        \n",
    "        # here is the example:\n",
    "        # position: [0, 1, 2, 3]\n",
    "        # i: [0, 2]\n",
    "        # div_term: [1, 100, 10000]\n",
    "        \n",
    "        # in order to calculate the div, we need transform position for boardcast.\n",
    "        # we will use the boardcast to fill the position_encoding matrix\n",
    "        # [0, 1, 2, 3] -> [0, 0, 0, 0]\n",
    "        # [0, 1, 2, 3] -> [1, 100, 10000, 1000000]\n",
    "        # [0, 1, 2, 3] -> [2, 200, 20000, 2000000]\n",
    "        # [0, 1, 2, 3] -> [3, 300, 30000, 3000000]\n",
    "        self.div_term = self.position / torch.pow(10000.0, 2 * i / d_model)\n",
    "        \n",
    "        # We will leverage boardcast to fill the position_encoding matrix\n",
    "        # the self.div_term is a matrix with shape (max_len, d_model/2)\n",
    "        # fill odd index with sin, even index with cos\n",
    "        self.position_encoding[:, 0::2] = torch.sin(self.div_term)\n",
    "        self.position_encoding[:, 1::2] = torch.cos(self.div_term)\n",
    "   \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)  # x.shape = (seq_len, d_model)\n",
    "        seq_len = min(self.max_len, seq_len)\n",
    "        x = x + self.position_encoding[:seq_len, :]\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.mha = MaskedMultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.add_norm_mha = TransformerAddNorm(d_model)\n",
    "        self.add_norm_ffn = TransformerAddNorm(d_model)        \n",
    "    \n",
    "    def forward(self, x, src_mask):\n",
    "        # x.shape = (seq_len, d_model)\n",
    "        x = self.add_norm_mha(x, lambda y: self.mha(y, y, y, src_mask))\n",
    "        x = self.add_norm_ffn(x, self.ffn)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, n_layers):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.mha_1 = MaskedMultiHeadAttention(d_model, n_heads)\n",
    "        self.add_norm_mha_1 = TransformerAddNorm(d_model)\n",
    "        self.corss_mha = MaskedMultiHeadAttention(d_model, n_heads)\n",
    "        self.add_norm_mha_2 = TransformerAddNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.add_norm_ffn = TransformerAddNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, src, src_mask, tgt_mask):\n",
    "        x = self.add_norm_mha_1(x, lambda y: self.mha_1(y, y, y, tgt_mask))\n",
    "        x = self.add_norm_mha_2(x, lambda y: self.corss_mha(y, src, src, src_mask))\n",
    "        x = self.add_norm_ffn(x, self.ffn)\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, n_layers):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, src, src_mask, tgt_mask):\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, src, src_mask, tgt_mask)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, n_layers):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, n_heads, d_ff, n_layers)\n",
    "        self.decoder = Decoder(d_model, n_heads, d_ff, n_layers)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        src = self.encoder(src, src_mask)\n",
    "        tgt = self.decoder(tgt, src, src_mask, tgt_mask)\n",
    "        out = self.linear(tgt)\n",
    "        return self.softmax(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 100 samples of length 16\n",
      "Source shape per sample: torch.Size([16, 1024])\n",
      "Target shape per sample: torch.Size([16, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Training Setup and Process\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def create_mask(seq_len):\n",
    "    \"\"\"Create a causal mask for decoder (prevent looking at future tokens)\"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask\n",
    "\n",
    "def generate_sample_data(seq_len, d_model, num_samples=32):\n",
    "    \"\"\"Generate synthetic training data - no batching, individual samples\"\"\"\n",
    "    # Generate individual samples as list of 2D tensors\n",
    "    src_data = [torch.randn(seq_len, d_model) for _ in range(num_samples)]\n",
    "    tgt_data = [torch.randn(seq_len, d_model) for _ in range(num_samples)]\n",
    "    \n",
    "    return src_data, tgt_data\n",
    "\n",
    "# Model hyperparameters\n",
    "d_model = 1024      # Model dimension\n",
    "n_heads = 8       # Number of attention heads  \n",
    "d_ff = 2048      # Feed-forward dimension\n",
    "n_layers = 8      # Number of encoder/decoder layers\n",
    "seq_len = 16      # Actual sequence length for training\n",
    "\n",
    "# Generate training data\n",
    "num_samples = 100\n",
    "src_data, tgt_data = generate_sample_data(seq_len, d_model, num_samples)\n",
    "\n",
    "print(f\"Training data: {num_samples} samples of length {seq_len}\")\n",
    "print(f\"Source shape per sample: {src_data[0].shape}\")\n",
    "print(f\"Target shape per sample: {tgt_data[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Transformer.__init__() takes 5 positional arguments but 6 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training Loop - No Batching (matches MHA implementation)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Training setup\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_ff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m      6\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "\u001b[0;31mTypeError\u001b[0m: Transformer.__init__() takes 5 positional arguments but 6 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop - No Batching (matches MHA implementation)\n",
    "\n",
    "# Training setup\n",
    "model = Transformer(d_model, n_heads, d_ff, n_layers)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Create masks\n",
    "src_mask = None  # No masking for encoder\n",
    "tgt_mask = create_mask(seq_len)  # Causal mask for decoder\n",
    "\n",
    "model.train()\n",
    "epochs = 50\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Process each sample individually (no batching)\n",
    "    for i in range(num_samples):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get single sample: (seq_len, d_model)\n",
    "        src_sample = src_data[i]\n",
    "        tgt_sample = tgt_data[i]\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src_sample, tgt_sample, src_mask, tgt_mask)\n",
    "        \n",
    "        # Compute loss (predicting the target sequence)\n",
    "        loss = criterion(output, tgt_sample)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / num_samples\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:2d}, Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n=== Model Evaluation ===\n",
      "Test input shape: torch.Size([16, 1024])\n",
      "Test target shape: torch.Size([16, 1024])\n",
      "Test output shape: torch.Size([16, 1024])\n",
      "Test loss: 0.995932\n",
      "Output sum per position (should be ~1.0): tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "\\n=== Training Summary ===\n",
      "✅ Transformer model with 169,051,136 parameters\n",
      "✅ Trained on 100 synthetic samples for 50 epochs\n",
      "✅ Final loss: 1.000419\n",
      "✅ Model successfully processes sequences of length 16\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation and Testing\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test on a single sample\n",
    "    test_src = torch.randn(seq_len, d_model)\n",
    "    test_tgt = torch.randn(seq_len, d_model)\n",
    "    \n",
    "    print(\"\\\\n=== Model Evaluation ===\")\n",
    "    print(f\"Test input shape: {test_src.shape}\")\n",
    "    print(f\"Test target shape: {test_tgt.shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(test_src, test_tgt, src_mask, tgt_mask)\n",
    "    test_loss = criterion(output, test_tgt)\n",
    "    \n",
    "    print(f\"Test output shape: {output.shape}\")\n",
    "    print(f\"Test loss: {test_loss.item():.6f}\")\n",
    "    \n",
    "    # Check if output is properly normalized (due to softmax)\n",
    "    print(f\"Output sum per position (should be ~1.0): {output.sum(dim=-1)[:5]}\")  # First 5 positions\n",
    "\n",
    "print(\"\\\\n=== Training Summary ===\")\n",
    "print(f\"✅ Transformer model with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"✅ Trained on {num_samples} synthetic samples for {epochs} epochs\")\n",
    "print(f\"✅ Final loss: {avg_loss:.6f}\")\n",
    "print(f\"✅ Model successfully processes sequences of length {seq_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
