{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer in image\n",
    "\n",
    "<img src=\"./assert/transformer.png\" width=\"50%\" height=\"50%\" alt=\"transformer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add musk function to multi-head attention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.w_query = nn.Linear(d_model, d_model)\n",
    "        self.w_key = nn.Linear(d_model, d_model)\n",
    "        self.w_value = nn.Linear(d_model, d_model)\n",
    "        self.attention_scores = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        seq_len = query.shape[0]\n",
    "        # from input, the query, key, value will be simply input matrix, input, input, input.\n",
    "        query = self.w_query(query)\n",
    "        key = self.w_key(key)\n",
    "        value = self.w_value(value)\n",
    "\n",
    "        query = query.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "        key = key.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "        value = value.view(seq_len, self.n_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        attention_scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # The computation is pretty similar to the previous one:\n",
    "            # we have seq_len of query ( sub matrix ), after processing query * key.T, we get\n",
    "            # seq_len, (n_heads, n_heads), where n_head is the size of original attention matrix\n",
    "            \n",
    "            # Note:\n",
    "            # Post mask, we don't want to have 0 as masked value,\n",
    "            # because, softmax(0) = 1, which will make the attention score too high.\n",
    "            # So, we use -inf to mask the value. The normalization will still remains.\n",
    "            attention_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attention = torch.matmul(torch.softmax(attention_scores, dim=-1), value)\n",
    "\n",
    "        attention = attention.transpose(0, 1).contiguous().view(seq_len, self.d_model)\n",
    "        return self.attention_scores(attention)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a component for add & Norm, it pretty much means x + layer_norm(x)\n",
    "\n",
    "class TransformerAddNorm(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "    \n",
    "    # Note the sublayer here is MultiHeadAttention\n",
    "    def forward(self, x, sublayer):\n",
    "        return self.layer_norm(x + sublayer(x))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.feed_forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.encoder_layers = nn.Sequential(\n",
    "            MultiHeadAttention(d_model, n_heads),\n",
    "            TransformerAddNorm(d_model),\n",
    "            FeedForward(d_model, d_ff),\n",
    "            TransformerAddNorm(d_model),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x.shape = (batch_size, seq_len, d_model)\n",
    "        return self.encoder_layers(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, max_seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.decoder_layers = nn.Sequential(\n",
    "            MultiHeadAttention(d_model, n_heads),\n",
    "            TransformerAddNorm(d_model),\n",
    "            MultiHeadAttention(d_model, n_heads),\n",
    "            TransformerAddNorm(d_model),\n",
    "            FeedForward(d_model, d_ff),\n",
    "            TransformerAddNorm(d_model),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder_layers(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
