{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In this section we are going to create a pytorch model for training.\n",
    "The model will be simply with a single headed self-attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Linear layers for query, key, and value, make input and output have the same dimension\n",
    "        self.w_query = nn.Linear(d_model, d_model)\n",
    "        self.w_key = nn.Linear(d_model, d_model)\n",
    "        self.w_value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention_scores = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is the input tensor\n",
    "        # x.shape = (seq_len, d_model) - no batch dimension\n",
    "        query = self.w_query(x)\n",
    "        key = self.w_key(x)\n",
    "        value = self.w_value(x)\n",
    "        \n",
    "        # Compute attention scores: (seq_len, seq_len)\n",
    "        attention_weights = torch.softmax(query @ key.T / math.sqrt(self.d_model), dim=-1)\n",
    "        \n",
    "        # Apply attention to values: (seq_len, d_model)\n",
    "        out = attention_weights @ value\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain\n",
    "\n",
    "Assume we have input like [\"I\", \"Love\", \"LLM\"]\n",
    "\n",
    "For each word, we do a tokenize process, so it will become size of **embed_dim**, in this case, our seq_len = 3\n",
    "\n",
    "On following example, we create a training logic.\n",
    "\n",
    "The lost func we pick is MSE, since the output will be a vector, regression will most likely fit into MSE bucket.\n",
    "\n",
    "## General practice\n",
    "\n",
    "In order to train the model, we have couple steps:\n",
    "\n",
    "1. prepare a **model**, **optmizer**, **lost function**, we feed model to optimizer, \n",
    "2. loop through a hyper-parameter **epoch**( mean, the value is hard-coded)\n",
    "3. for each epoch, we do\n",
    "    a. clear the grad() <- so previous calculated grad wont impact this epoch\n",
    "    b. get output from model\n",
    "    c. get loss from output and target\n",
    "    d. process a back-propagation by call loss.backward(), this will follow chain rule to reflect grad to first layer.\n",
    "    e. use optimizer.step() to update the parameters of model\n",
    "\n",
    "## Dive Deep\n",
    "\n",
    "1. Why the backward() applied on loss instead of model ?\n",
    "\n",
    "This is the way how pytorch works.\n",
    "\n",
    "The **model** saved a lot of layers, activation, nn etc...\n",
    "While **loss** contains meaningful scalar value. Why? well, normally loss is a simply value like torch.tensor(1), because the loss normally will be a single number after calculation of MSE.\n",
    "\n",
    "With these two concept, **model** saved entire layers, and it is a graph, while loss, is the end of current graph, this will be the start place for back-propagation.\n",
    "\n",
    "Following is an example for 3-layer chain\n",
    "\n",
    "## Example: Simple 3-layer chain\n",
    "\n",
    "Suppose:\n",
    "\n",
    "$$\n",
    "\\text{loss} = f(z), \\quad z = g(y), \\quad y = h(\\theta)\n",
    "$$\n",
    "\n",
    "To find:\n",
    "\n",
    "$$\n",
    "\\frac{d\\, \\text{loss}}{d\\theta}\n",
    "$$\n",
    "\n",
    "We apply the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{d\\, \\text{loss}}{d\\theta} = \\frac{d\\, \\text{loss}}{dz} \\times \\frac{dz}{dy} \\times \\frac{dy}{d\\theta}\n",
    "$$\n",
    "\n",
    "\n",
    "## Qestion\n",
    "\n",
    "1. In what case we dont need clear grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.9530\n",
      "Epoch 10, Loss: 0.8593\n",
      "Epoch 20, Loss: 0.7912\n",
      "Epoch 30, Loss: 0.7181\n",
      "Epoch 40, Loss: 0.6358\n",
      "Epoch 50, Loss: 0.5504\n",
      "Epoch 60, Loss: 0.4689\n",
      "Epoch 70, Loss: 0.3945\n",
      "Epoch 80, Loss: 0.3288\n",
      "Epoch 90, Loss: 0.2758\n",
      "Epoch 100, Loss: 0.2360\n",
      "Epoch 110, Loss: 0.2062\n",
      "Epoch 120, Loss: 0.1824\n",
      "Epoch 130, Loss: 0.1621\n",
      "Epoch 140, Loss: 0.1441\n",
      "Epoch 150, Loss: 0.1277\n",
      "Epoch 160, Loss: 0.1127\n",
      "Epoch 170, Loss: 0.0989\n",
      "Epoch 180, Loss: 0.0863\n",
      "Epoch 190, Loss: 0.0742\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "embed_dim = 32\n",
    "seq_len = 8\n",
    "\n",
    "# No batching: Create 2D input tensor (seq_len, d_model)\n",
    "x = torch.randn(seq_len, embed_dim)\n",
    "target = torch.randn(seq_len, embed_dim)\n",
    "\n",
    "model = SelfAttention(embed_dim)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
